{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following data:\n",
    "$$ X = \\begin{bmatrix} 1 & 1 & 2 \\\\ 1 & 2 & 1 \\\\ 1 & 1 & 1 \\end{bmatrix}, \\quad y = \\begin{bmatrix} 11 \\\\ 10 \\\\ 8 \\end{bmatrix}$$\n",
    "where $X$ is the data matrix and $y$ contains the labels.\n",
    "\n",
    "We want to find the parameter vector \n",
    "$$\\beta = \\begin{bmatrix} \\beta_1 & \\beta_2 & \\beta_3 \\end{bmatrix}^T$$ \n",
    "that minimizes the loss over all instances $x_i$ (the i-th row of the matrix X):\n",
    "$$ L(X, \\beta, y) = \\sum_{i=1}^3(\\beta^Tx_i - y_i)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Explain the differences between the classical gradient method (GD) and the stochastic gradient method (SGD).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classical GD method is an iterative optimization (minimization/maximization) method that uses the gradient of the function as the direction (for maximization problems, the opposite direction for minimization ones) toward which to update the approximate solution: the gradient is computed by approximating its value by using the whole dataset available at each step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, in the SGD we approximate the gradient by only using a subset of points of the dataset, chosen randomly. It can be proven that the SGD yields the same results as the classical GD, but it is computationally more efficient, since we use less data to approximate the gradient of the function at each step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many types of SGD: with or without replacement of the subset of points of the datased used at each iteration, using only one point or a batch of points as subset, introducing \"momentum\" of the iteration, where we also consider the past directions to update the next, and many more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Perform two epochs using SGD with a step size $\\eta = 0.1$ and report the errors and the total loss after each epoch; use the initial guess $\\beta = \\begin{bmatrix} 1 & 1 & 1 \\end{bmatrix}^T$ . (Run through the instances in order instead of performing a random selection.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax \n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(X, y, beta,):\n",
    "    return jnp.sum(jnp.square(X @ beta - y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1.0, 1.0, 2.0], [1.0, 2.0, 1.0], [1.0, 1.0, 1.0]])\n",
    "y = np.array([11.0, 10.0, 8.0]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_jit = jax.jit(loss)\n",
    "grad_jit = jax.jit(jax.grad(loss, argnums=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_params():\n",
    "    return np.array([1.0, 1.0, 1.0]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110.0\n",
      "1113.0244\n",
      "[-1.8000007 -3.040001  -2.6800013]\n"
     ]
    }
   ],
   "source": [
    "params = initialize_params()\n",
    "learning_rate = 0.1\n",
    "n_epochs = 2\n",
    "\n",
    "print(loss_jit(X, y, params))\n",
    "for i in range(n_epochs):\n",
    "    grad = grad_jit(X, y, params)\n",
    "    params -= learning_rate * grad\n",
    "\n",
    "print(loss_jit(X, y, params))\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, with the classical GD the method is diverging. This is probably due to the learning rate too high (by reducing it to $\\eta = 0.01$ we obtain convergence)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "340.0\n",
      "33.52\n",
      "[[2.2]\n",
      " [2. ]\n",
      " [3.6]]\n"
     ]
    }
   ],
   "source": [
    "beta = np.array([[1.0, 1.0, 1.0]]).T\n",
    "learning_rate = 0.1\n",
    "n_epochs = 2\n",
    "\n",
    "print(loss_jit(X, y, beta))\n",
    "for epoch in range(n_epochs):\n",
    "    i = epoch % X.shape[0] # indice per il dataset\n",
    "\n",
    "    grad = grad_jit(X[i], y[i], beta)\n",
    "    beta -= learning_rate * grad\n",
    "\n",
    "print(loss_jit(X, y, beta))\n",
    "print(beta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time it's converging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADAGRAD (ADAptive GRADient) is a variation of the SGD method in which the learning rate is adapted for each feature in the dataset (3 feature in this case): it assigns a higher learning rate to infrequent features, and a smaller learning rate to features that appears very frequently.\n",
    "\n",
    "I don't think that in this case ADAGRAD would be helpful due to the small size of features in the dataset and the experimental fact that the plain SGD also performs very well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110.0\n",
      "89.49256\n",
      "[1.1624695 1.1847999 1.1371391]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "beta = initialize_params()\n",
    "delta = 1.0e-7\n",
    "n_epochs = 2\n",
    "learning_rate = 0.1\n",
    "print(loss_jit(X, y, beta))\n",
    "r = np.zeros(beta.shape)\n",
    "for epoch in range(n_epochs):\n",
    "    i = epoch % X.shape[0]\n",
    "\n",
    "    grad = grad_jit(X[i], y[i], beta)\n",
    "    r += grad * grad\n",
    "    beta -= learning_rate / (delta + np.sqrt(r)) * grad\n",
    "\n",
    "\n",
    "print(loss_jit(X, y, beta))\n",
    "print(beta)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
