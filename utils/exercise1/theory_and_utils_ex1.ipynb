{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Compute the reconstruction error of the dataset as a function of $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "U, s, VT = np.linalg.svd(A_norm, full_matrices=False)\n",
    "\n",
    "reconstruction_error = list()\n",
    "ks = np.array()\n",
    "\n",
    "for k in ks:\n",
    "    A_k = U[:, :k] @ np.diag(s[:k]) @ VT[:k, :]\n",
    "    reconstruction_error.append(np.linalg.norm(A_norm - A_k, ord=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Reconstruction Error**\n",
    "\n",
    "Reconstruction error measures how well a reduced representation of the data (using fewer principal components) approximates the original data. It is calculated as the difference between the original dataset $A_{\\text{bar}}$ and the approximated dataset $A_k$, which is reconstructed using only the top $k$ singular values and vectors.\n",
    "\n",
    "The error is computed as:\n",
    "$$\n",
    "\\text{Reconstruction Error} = \\| A_{\\text{bar}} - A_k \\|_2\n",
    "$$\n",
    "\n",
    "Here:\n",
    "- $A_{\\text{bar}}$: The original normalized dataset.\n",
    "- $A_k$: The reconstructed dataset using $k$ principal components.\n",
    "\n",
    "#### **3. `ord=2` in `np.linalg.norm`**\n",
    "\n",
    "In `np.linalg.norm`, the parameter `ord` specifies the type of norm to compute. When `ord=2`:\n",
    "- It computes the **spectral norm** (largest singular value of the difference matrix).\n",
    "- Mathematically, it is:\n",
    "$$\n",
    "\\| A_{\\text{bar}} - A_k \\|_2 = \\sigma_{\\text{max}}\n",
    "$$\n",
    "where $\\sigma_{\\text{max}}$ is the largest singular value of $A_{\\text{bar}} - A_k$.\n",
    "\n",
    "In this context, `ord=2` provides a measure of the largest deviation between the original data and the reconstructed data along any dimension.\n",
    "\n",
    "- It aligns with the concept of variance maximization along principal components.\n",
    "- It ensures the reconstruction error focuses on the direction with the largest error, providing a conservative measure of reconstruction quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Principal Component Analysis is a dimensionality reduction technique used to transform high-dimensional data into a lower-dimensional representation, by means of linear combinations of vectors that are directed in the directions where variance is maximized, in descending order: this means that the vector related to the first principal component is the direction of maximum variance in the dataset, the vector related to the second principal component is the second direction where variance is maximized, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the steps required to perform PCA:\n",
    "- Center (or standardize) the data: move to a dataset where each feature has zero mean (and unit variance if standardized);\n",
    "- Compute the covariance matrix of the centered (standardized) data: $$ C = \\frac{A^TA}{n-1}$$ with $n$ number of samples;\n",
    "- Find eigenvectors and eigenvalues of the covariance matrix: $C$ is symmetric, so it can be diagonalized $$C = WLW^T$$ Eigenvectors are in matrix $W$ and represent the principal components (direction of maximum variance), and the eigenvalues are in the diagonal matrix $L$ and indicate the amount of variance explained by each principal component (the magnitude of the variance). \n",
    "\n",
    "    Eigenvectors are all orthogonal to each other, and the corresponding eigenvalues are sorted in descending order;\n",
    "- Project the data onto the new feature space: principal components for the data are given by $AV$. \n",
    "\n",
    "    The new features will be a linear combination of the original features. The projection matrix is obtained by stacking the eigenvectores corresponding to the selected principal components (i.e., the ones that together explain the majority of the variance in the dataset). The new dataset is obtained by multiplying the standardized data by the projection matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVD can be used in this process by modifying a little bit the steps:\n",
    "- Instead of computing the covariance matrix of the centered (standardized) data, apply the SVD decomposition on it: $$A = U\\Sigma V^T$$\n",
    " Matrix $U$ contains the left singular vectors, matrix $V^T$ contains the right singular vectors, while matrix $\\Sigma$ is diagonal and contains the singular values.\n",
    " We can easily see that $$C = \\frac{V\\Sigma U^TU\\Sigma V^T}{n-1} = V\\frac{\\Sigma ^2}{n-1}V^T$$ so right singular vectors $V$ are principal components and singular values are related to eigenvalues of the covariance matrix by $\\lambda _i = \\frac{\\sigma _i ^2}{n-1} $ .\n",
    "- Principal components are then obtained by multiplying $AV = U\\Sigma V^TV = U\\Sigma$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform PCA on the dataset by means of the SVD decomposition. Then, plot the trend of:\n",
    "- The **singular values**: $$\\sigma_k$$\n",
    "- The **cumulate fraction of singular values**: $$\\frac{\\sum_{i=1}^k \\sigma_i}{\\sum_{i=1}^q \\sigma_i}$$\n",
    "- The **fraction of the explained variance**: $$\\frac{\\sum_{i=1}^k \\sigma_i^2}{\\sum_{i=1}^q \\sigma_i^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "A_norm = (A - np.mean(A, axis=1)[:,None]) / np.std(A, axis=1)[:,None]\n",
    "U, s, VT = np.linalg.svd(A_norm, full_matrices=False)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize = (18,6))\n",
    "\n",
    "axes[0].semilogy(s, 'o-')\n",
    "axes[0].set_title('Singular Values')\n",
    "\n",
    "axes[1].plot(np.cumsum(s) / np.sum(s), 'o-')\n",
    "axes[1].set_title('Cumulate fraction of singular values')\n",
    "\n",
    "axes[2].plot(np.cumsum(s**2) / np.sum(s**2), 'o-')\n",
    "axes[2].set_title('Explained Variance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a scatterplot of the first two principal components of the dataset, grouped by label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "for g in groups:\n",
    "  plt.scatter(Phi[0, labels == g], Phi[1, labels == g], label = g)\n",
    "plt.xlabel('1st p.c.')\n",
    "plt.ylabel('2nd p.c.')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rSVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomized SVD (RSVD) is a computationally efficient alternative to traditional SVD, particularly useful for large datasets. RSVD approximates the singular values and singular vectors of a matrix by projecting the data onto a lower-dimensional subspace, reducing the computational cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rsvd is a quick way of computing A reduced version of the SVD when you don't want all the singular values in the associated singular values, but just those up to some rank K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def randomized_SVD(A, k):\n",
    "  m, n = A.shape\n",
    "  P = np.random.randn(n, k) #generates random numbers according to the standard normal distribution\n",
    "  '''\n",
    "  you want to extract \n",
    "  the most prominent features \n",
    "  that are in the columns\n",
    "  '''\n",
    "  Z = A @ P\n",
    "  Q, R = np.linalg.qr(Z)\n",
    "  Y = Q.T @ A # QR factorization\n",
    "  Uy, sy, VTy = np.linalg.svd(Y, full_matrices = False)\n",
    "  U = Q @ Uy\n",
    "  return U, sy, VTy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RSVD with oversampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In RSVD, oversampling involves adding extra dimensions (columns) to the random projection matrix used to approximate the input matrix. The purpose of oversampling is to improve the accuracy of the approximation by ensuring that important features of the input matrix are not lost.\n",
    "\n",
    "1. **Random Projection Matrix**:\n",
    "   - RSVD starts by generating a random projection matrix $P$ with dimensions $n \\times (k + p)$, where:\n",
    "     - $k$: Target rank (number of singular values/vectors desired).\n",
    "     - $p$: Oversampling parameter (additional dimensions added to $P$).\n",
    "   - $p$ helps capture variability that might otherwise be missed if the rank $k$ is too small.\n",
    "\n",
    "2. **Oversampling Formula**:\n",
    "   $$\n",
    "   p = \\lceil 0.5k \\rceil\n",
    "   $$\n",
    "   - 50% more dimensions than the target rank $k$ are added to ensure better approximation.\n",
    "\n",
    "3. **Steps in RSVD with Oversampling**:\n",
    "   - Generate a random matrix $P$ of size $n \\times (k + p)$.\n",
    "   - Form the projected matrix $B = A P$, where $A$ is the input matrix.\n",
    "   - Perform SVD on the smaller matrix $B$ to approximate the singular values and singular vectors of $A$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# default oversampling_size=1.5 is 50% of oversampling\n",
    "def randomized_SVD_oversampling(A, k, oversampling_size=1.5):\n",
    "    np.random.seed(random_seed)\n",
    "    m, n = A.shape\n",
    "    # create a random matrix\n",
    "    omega = np.random.rand(n, round(k * oversampling_size))\n",
    "    Y = A @ omega\n",
    "    Q, R = np.linalg.qr(Y)\n",
    "    B = Q.T @ A\n",
    "    # perform the SVD on B\n",
    "    rU, rs, rVT = np.linalg.svd(B, full_matrices=False)\n",
    "    # recover left singular values\n",
    "    rU = Q @ rU\n",
    "    return rU, rs, rVT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General comments of what we see/achieve\n",
    "\n",
    "- We can see that as the rank k of the randomized SVD algorithm increases, the approximation of the singular values becomes more and more accurate.\n",
    "There is an overall underestimation of the singular values (i.e. less variability is captured), but the approximation is still very good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVT "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reconstruct images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "mask_keep = [] #implement it to know which elements are correct, and so we need to keep\n",
    "def SVT(img_ch, s_treshold):\n",
    "    max_iter= 200\n",
    "    tol= 1.0e-6\n",
    "\n",
    "    img = img_ch.copy() # to copy without creating a references (because if not when we modify x hat we modify also x full)\n",
    "    for k in range(max_iter):\n",
    "        img_old = img.copy() # A copy of the current X_hat to track changes between iterations\n",
    "        U,s,VT = np.linalg.svd(img, full_matrices = False)\n",
    "        s[s < s_treshold] = 0 #mask for thresholding, so we can get rid of the singular values below the threshold\n",
    "        img = U @ np.diag(s) @ VT # reconstuct using the multiplication of the components of the svd with the sigma truncated\n",
    "        img[mask_keep] = img_ch[mask_keep] #put in the positions where we have the data, the correct data \n",
    "        increment = np.linalg.norm(img - img_old)\n",
    "\n",
    "        if increment < tol:\n",
    "            print(\"Toleranche achieved!\")\n",
    "            break\n",
    "\n",
    "        print('======= iteration %d (increment %1.2e)' % (k, increment))\n",
    "\n",
    "    return img"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
