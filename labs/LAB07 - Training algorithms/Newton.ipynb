{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bZCDNUgw0nI3"
   },
   "source": [
    "# Newton method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "IJcJCwFhc8cs"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "\n",
    "# We enable double precision in JAX\n",
    "from jax import config\n",
    "config.update(\"jax_enable_x64\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7qkdJdGg1AP1"
   },
   "source": [
    "We consider a random matrix $A \\in \\mathbb{R}^{n\\times n}$, with $n = 100$ and a random vector $\\mathbf{x}_{\\text{ex}} \\in \\mathbb{R}^n$.\n",
    "We define then $\\mathbf{b} = A \\, \\mathbf{x}_{\\text{ex}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "c0h8ihCddDPf"
   },
   "outputs": [],
   "source": [
    "n = 100\n",
    "\n",
    "np.random.seed(0)\n",
    "A = np.random.randn(n,n)\n",
    "x_ex = np.random.randn(n)\n",
    "b = A @ x_ex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UanVhF4xAVoX"
   },
   "source": [
    "Define the loss function\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\mathbf{x}) = \\| \\mathbf{b} - A \\, \\mathbf{x} \\|_2^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(x):\n",
    "    return jnp.sum(jnp.square( A @ x - b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uAZ9XGaiAs3X"
   },
   "source": [
    "By using the `jax` library, implement and compile functins returning the gradient ($\\nabla \\mathcal{J}(\\mathbf{x})$) and the hessian ($\\nabla^2 \\mathcal{J}(\\mathbf{x})$) of the loss function (*Hint*: use the `jacrev` or the `jacfwd`) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "KflmuLXld2T4"
   },
   "outputs": [],
   "source": [
    "grad = jax.grad(loss)\n",
    "hess = jax.jacfwd(jax.jacrev(loss))\n",
    "\n",
    "loss_jit = jax.jit(loss)\n",
    "grad_jit = jax.jit(grad)\n",
    "hess_jit = jax.jit(hess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bSMg8ocDBndO"
   },
   "source": [
    "Check that the results are correct (up to machine precision)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "xZulGRQ1efFP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1550089998180016e-12\n",
      "4.829664679334261e-13\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "x_guess = np.random.randn(n)\n",
    "\n",
    "G_ad = grad_jit(x_guess)\n",
    "G_ex = 2 * A.T @ (A @ x_guess - b)\n",
    "print(np.linalg.norm(G_ad - G_ex))\n",
    "\n",
    "H_ad = hess_jit(x_guess)\n",
    "H_ex = 2 * A.T @ A\n",
    "print(np.linalg.norm(H_ad - H_ex))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-gA_kKPB2SV"
   },
   "source": [
    "Exploit the formula\n",
    "$$\n",
    "\\nabla^2 \\mathcal{J}(\\mathbf{x}) \\mathbf{v} = \\nabla_{\\mathbf{x}} \\phi(\\mathbf{x}, \\mathbf{v})\n",
    "$$\n",
    "where \n",
    "$$\n",
    "\\phi(\\mathbf{x}, \\mathbf{v}) := \\nabla \\mathcal{J}(\\mathbf{x}) \\cdot \\mathbf{v}\n",
    "$$\n",
    "to write an optimized function returning the hessian-vector-product\n",
    "$$\n",
    "(\\mathbf{x}, \\mathbf{v}) \\mapsto \\nabla^2 \\mathcal{J}(\\mathbf{x}) \\mathbf{v}.\n",
    "$$\n",
    "Compare the computational performance w.r.t. the full hessian computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why This Formula Works\n",
    "This formula relies on the fact that differentiation is a linear operator. By chaining gradients, we compute exactly the derivative needed for the Hessian-vector product without ever constructing the Hessian itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T9969dU4kc6f",
    "outputId": "368b2173-e971-474c-e3b1-f649d9bb15d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2744887647117243e-12\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "v = np.random.randn(n)\n",
    "\n",
    "hvp_basic = lambda x, v: hess(x) @ v\n",
    "gvp = lambda x,v : jnp.dot(grad(x), v)\n",
    "hvp = lambda x,v : jax.grad(gvp, argnums=0)(x,v)\n",
    "\n",
    "hvp_basic_jit = jax.jit(hvp_basic)\n",
    "hvp_jit = jax.jit(hvp)\n",
    "\n",
    "Hv_ad = hvp_jit(x_guess, v)\n",
    "Hv_ex = H_ex @ v\n",
    "print(np.linalg.norm(Hv_ad - Hv_ex))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "jsA4eUnuj3ju"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172 µs ± 24.9 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "17.3 µs ± 4.96 µs per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit hvp_basic_jit(x_guess, v)\n",
    "%timeit hvp_jit(x_guess, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TagmrdjG4Ww4"
   },
   "source": [
    "Implement the Newton method for the minimization of the loss function $\\mathcal{L}$. Set a maximim number of 100 iterations and a tolerance on the increment norm of $\\epsilon = 10^{-8}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x_guess.copy()\n",
    "num_epochs = 100\n",
    "eps = 1e-8\n",
    "\n",
    "elapsed_time = 0\n",
    "for epoch in range(num_epochs):\n",
    "    t0 = time.time()\n",
    "    # compute the gradient G and the increment incr by using the CONIUATE GRADIENT\n",
    "    G = grad_jit(x)\n",
    "    H = hess_jit(x)\n",
    "    incr = np.linalg.solve(H, -G) # if you put -G you can su,the incr\n",
    "    # YOU SHOULD NEVER COMPUTE EXPLICITLY THE HESSIAN\n",
    "    elapsed_time += time.time() - t0\n",
    "    x += incr\n",
    "\n",
    "    print(\"========== epoch %d\" % epoch)\n",
    "    print(\"loss: %1.3e\" % loss_jit(x))\n",
    "    print(\"grad: %1.3e\" % np.linalg.norm(G))\n",
    "    print(\"loss: %1.3e\" %  np.linalg.norm(incr))\n",
    "\n",
    "    if np.linalg.norm(incr) < eps:\n",
    "        break\n",
    "\n",
    "\n",
    "print(f\"Elapsed time: {elapsed_time:.4f} [s]\")\n",
    "real_err = np.linalg.norm(x -x_ex) / np.linalg.norm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uNL7303C4oTL"
   },
   "source": [
    "Repeat the optimization loop for the loss function\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\mathbf{x}) = \\| \\mathbf{b} - A \\, \\mathbf{x} \\|_4^4\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
